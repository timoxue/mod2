# -*- coding: utf-8 -*-
import re
import csv
import pdfplumber
from pathlib import Path
from typing import List, Dict
from config import PDF_FILE, CSV_DIR


#PDF_FILE = "åŒ–å­¦è¯å“ä»¿åˆ¶è¯ä¸Šå¸‚è®¸å¯ç”³è¯·æ¨¡å—äºŒè¯å­¦èµ„æ–™æ’°å†™è¦æ±‚ï¼ˆåˆ¶å‰‚ï¼‰ï¼ˆè¯•è¡Œï¼‰.pdf"
OUTPUT_DIR = CSV_DIR

def extract_sections_and_content(pdf_path: str):
    """é€šç”¨æå–å™¨ï¼šç« èŠ‚ã€å…³æ³¨ç‚¹ã€è¡¨æ ¼ï¼ˆæŒ‰é¡µç å½’å±ï¼‰"""
    with pdfplumber.open(pdf_path) as pdf:
        pages_text = [(i+1, page.extract_text(x_tolerance=1, y_tolerance=1) or "") for i, page in enumerate(pdf.pages)]
        pages_tables = [(i+1, page.extract_tables()) for i, page in enumerate(pdf.pages)]

    # åˆå¹¶å…¨æ–‡è¡Œï¼ˆå¸¦é¡µç ï¼‰
    all_lines = []
    for page_num, text in pages_text:
        for line in text.split('\n'):
            line = line.strip()
            if line:
                all_lines.append((page_num, line))

    # è¯†åˆ«ç« èŠ‚ï¼ˆé€šç”¨æ­£åˆ™ï¼‰
    section_pattern = r'^(\d+\.\d+\.P\.\d+(?:\.\d+)*)(?:\s+)(.+)$'
    sections = []
    current = None

    for page_num, line in all_lines:
        if re.match(r'^\d+\.\d+\.P$', line):  # è·³è¿‡ "2.3.P"
            continue
        match = re.match(section_pattern, line)
        if match:
            sec_id = match.group(1).strip()
            title = match.group(2).strip()
            current = {"id": sec_id, "title": title, "start_page": page_num, "focus": "", "tables": []}
            # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç›¸åŒ id çš„ç« èŠ‚
            existing = None
            for i, sec in enumerate(sections):
                if sec["id"] == current["id"]:
                    existing = i
                    break

            if existing is not None:
                # å·²å­˜åœ¨ç›¸åŒ id çš„ç« èŠ‚
                existing_sec = sections[existing]
                # åˆ¤æ–­å“ªæ¡æ ‡é¢˜æ›´å®Œæ•´ï¼ˆä¸å« '....'ï¼‰
                if "...." in existing_sec["title"] and "...." not in current["title"]:
                    # æ—§æ ‡é¢˜å« '....'ï¼Œæ–°æ ‡é¢˜å®Œæ•´ â†’ æ›¿æ¢
                    sections[existing] = current
                    current = None
                elif "...." in current["title"] and "...." not in existing_sec["title"]:
                    # æ–°æ ‡é¢˜å« '....'ï¼Œæ—§æ ‡é¢˜å®Œæ•´ â†’ ä¸¢å¼ƒæ–°è®°å½•ï¼Œä¸åš append
                    current = None
                else:
                    # ä¸¤è€…éƒ½å«æˆ–éƒ½ä¸å« '....'ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªï¼ˆæˆ–å¯åˆå¹¶ï¼Œæ­¤å¤„ä¿ç•™åŸé€»è¾‘ï¼‰
                    current = None

            # åªæœ‰ current æœ‰æ•ˆæ—¶æ‰ append
            if current is not None:
                sections.append(current)
        elif current and "ã€å…³æ³¨ç‚¹ã€‘" in line:
            # æ”¶é›†å…³æ³¨ç‚¹ï¼ˆç›´åˆ°ä¸‹ä¸€ç« èŠ‚ï¼‰
            idx = all_lines.index((page_num, line))
            focus = ""
            for p, l in all_lines[idx+1:]:
                if re.match(section_pattern, l) or l.startswith("2.3.P"):
                    break
                if l and "ã€å…³æ³¨ç‚¹ã€‘" not in l:
                    focus += l + " "
            current["focus"] = re.sub(r'\s+', ' ', focus).strip()

    # å…³è”è¡¨æ ¼ï¼ˆæŒ‰é¡µç åŒºé—´ï¼‰
    for i, sec in enumerate(sections):
        start = sec["start_page"]
        end = sections[i+1]["start_page"] - 1 if i+1 < len(sections) else float('inf')
        tables_in_section = []
        for page_num, tables in pages_tables:
            if start <= page_num <= end:
                for tbl in tables:
                    if tbl and len(tbl) > 1:
                        tables_in_section.append(tbl)
        sec["tables"] = tables_in_section

    return sections

def generate_csvs(sections: List[Dict], output_dir: str):
    """ç”Ÿæˆ regulations.csv, sections.csv, requirements.csv, checkpoints.csv"""
    Path(output_dir).mkdir(exist_ok=True)

    # 1. regulations.csv
    with open(f"{output_dir}/regulations.csv", "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, ["id", "name", "authority", "publish_date"])
        writer.writeheader()
        writer.writerow({
            "id": "NMPA_MODULE2_2025",
            "name": "åŒ–å­¦è¯å“ä»¿åˆ¶è¯ä¸Šå¸‚è®¸å¯ç”³è¯·æ¨¡å—äºŒè¯å­¦èµ„æ–™æ’°å†™è¦æ±‚ï¼ˆåˆ¶å‰‚ï¼‰ï¼ˆè¯•è¡Œï¼‰",
            "authority": "NMPA",
            "publish_date": "2025-08"
        })

    # 2. sections.csv
    with open(f"{output_dir}/sections.csv", "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, ["id", "regulation_id", "title"])
        writer.writeheader()
        for sec in sections:
            writer.writerow({
                "id": f"SEC_{sec['id'].replace('.', '_')}",
                "regulation_id": "NMPA_MODULE2_2025",
                "title": f"{sec['id']} {sec['title']}"
            })

    # 3. requirements.csv & 4. checkpoints.csv
    requirements = []
    checkpoints = []

    for sec in sections:
        sec_id_clean = sec["id"].replace(".", "_")
        sec_id_neo4j = f"SEC_{sec_id_clean}"
        req_id = f"REQ_{sec_id_clean}"
        focus = sec["focus"]

        # 3.1 è¯­ä¹‰ç±» requirement & checkpoint
        if focus:
            requirements.append({"id": req_id, "section_id": sec_id_neo4j, "text": focus})
            draft = f"æ˜¯å¦{focus.rstrip('ã€‚')}ï¼Ÿ" if not focus.endswith("ï¼Ÿ") else focus
            checkpoints.append({
                "id": f"CHK_{sec_id_clean}_FOCUS",
                "requirement_id": req_id,
                "text": draft,
                "ctd_location": sec["id"],
                "severity": "High"
            })

        # 3.2 è¡¨æ ¼ç±» checkpointï¼ˆå®Œå…¨åŸºäºçœŸå®è¡¨å¤´ï¼‰
        for table in sec["tables"]:
            header = table[0]
            clean_header = [str(h).strip() for h in header if h and str(h).strip()]
            if len(clean_header) >= 2:
                cols_str = "ã€".join([f"â€˜{col}â€™" for col in clean_header[:5]])  # æœ€å¤š5åˆ—
                table_draft = f"æ˜¯å¦æä¾›{sec['title']}ç›¸å…³çš„ç»“æ„åŒ–è¡¨æ ¼ï¼Œä¸”åŒ…å«åˆ—ï¼š{cols_str}ï¼Ÿ"
                # è¡¨æ ¼ç±» requirement å¯å¤ç”¨è¯­ä¹‰ç±»ï¼Œæˆ–å•ç‹¬åˆ›å»ºï¼ˆæ­¤å¤„å¤ç”¨ï¼‰
                req_id_table = req_id if focus else f"REQ_{sec_id_clean}_TABLE"
                if not focus:
                    requirements.append({"id": req_id_table, "section_id": sec_id_neo4j, "text": "è¡¨æ ¼ç»“æ„è¦æ±‚"})
                checkpoints.append({
                    "id": f"CHK_{sec_id_clean}_TABLE_{len(checkpoints)}",
                    "requirement_id": req_id_table,
                    "text": table_draft,
                    "ctd_location": sec["id"],
                    "severity": "Medium"
                })

    # å†™å…¥ requirements.csv
    with open(f"{output_dir}/requirements.csv", "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, ["id", "section_id", "text"])
        writer.writeheader()
        writer.writerows(requirements)

    # å†™å…¥ checkpoints.csv
    with open(f"{output_dir}/checkpoints.csv", "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.DictWriter(f, ["id", "requirement_id", "text", "ctd_location", "severity"])
        writer.writeheader()
        writer.writerows(checkpoints)

    print(f"âœ… å·²ç”Ÿæˆ 4 ä¸ª CSV æ–‡ä»¶ï¼Œä¿å­˜è‡³: {output_dir}/")
    print(f"   - regulations.csv: 1 æ¡")
    print(f"   - sections.csv: {len(sections)} æ¡")
    print(f"   - requirements.csv: {len(requirements)} æ¡")
    print(f"   - checkpoints.csv: {len(checkpoints)} æ¡")

# ================== ä¸»ç¨‹åº ==================
if __name__ == "__main__":
    if not Path(PDF_FILE).exists():
        raise FileNotFoundError(f"è¯·ç¡®ä¿ PDF æ–‡ä»¶å­˜åœ¨: {PDF_FILE}")

    print("ğŸ” æ­£åœ¨è§£æ PDFï¼Œæå–ç« èŠ‚ã€å…³æ³¨ç‚¹å’Œè¡¨æ ¼...")
    sections = extract_sections_and_content(PDF_FILE)
    generate_csvs(sections, OUTPUT_DIR)