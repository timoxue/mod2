# -*- coding: utf-8 -*-
"""
é€’å½’è§£æã€Šæ¨¡å—äºŒæ’°å†™è¦æ±‚ã€‹PDFï¼Œè¾“å‡ºç»“æ„åŒ– JSON
æ”¯æŒ L1-L6 å±‚çº§ã€ç‰¹æ®Šå—è¯†åˆ«ã€å†…éƒ¨å¼•ç”¨æå–ã€å­æ¡æ¬¾è¯†åˆ«
è¾“å‡ºæ ¼å¼ç¬¦åˆæŒ‡å®š schema
"""
import re
import json
from pathlib import Path
import pdfplumber

PDF_FILE = Path("åŒ–å­¦è¯å“ä»¿åˆ¶è¯ä¸Šå¸‚è®¸å¯ç”³è¯·æ¨¡å—äºŒè¯å­¦èµ„æ–™æ’°å†™è¦æ±‚ï¼ˆåˆ¶å‰‚ï¼‰ï¼ˆè¯•è¡Œï¼‰.pdf")
OUTPUT_JSON = Path("module2_structured.json")


def main():
    if not PDF_FILE.exists():
        raise FileNotFoundError(f"âŒ PDF æ–‡ä»¶ä¸å­˜åœ¨: {PDF_FILE}")

    print("ğŸ” æ­£åœ¨è§£æ PDF...")
    sections = parse_pdf_to_sections(PDF_FILE)
    tree = build_section_tree(sections)

    with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
        json.dump(tree, f, ensure_ascii=False, indent=2)

    print(f"âœ… ç»“æ„åŒ– JSON å·²ä¿å­˜è‡³: {OUTPUT_JSON}")


def parse_pdf_to_sections(pdf_path: Path):
    """è§£æ PDFï¼Œè¿”å›æ‰å¹³åŒ–ç« èŠ‚åˆ—è¡¨"""
    with pdfplumber.open(pdf_path) as pdf:
        pages = []
        for i, page in enumerate(pdf.pages):
            text = page.extract_text(x_tolerance=1, y_tolerance=1) or ""
            tables = page.extract_tables() or []
            pages.append((i+1, text, tables))

    all_lines = []
    for page_num, text, _ in pages:
        for line in text.split('\n'):
            line = line.strip()
            if line:
                all_lines.append((page_num, line))

    # æå–ç« èŠ‚ç¼–å·è¡Œï¼ˆæ”¯æŒ L3-L6ï¼‰
    section_id_pattern = r'^(\d+\.\d+\.P\.\d+(?:\.\d+)*)(?:\s+)(.+).*'
    section_headers = []
    for i, (page_num, line) in enumerate(all_lines):
        match= re.match(section_id_pattern, line)
        if match:
            sec_id = match.group(1).strip()
            title = match.group(2).strip()

            title = "..."
            if i + 1 < len(all_lines):
                next_line = all_lines[i+1][1]
                if (not re.match(section_id_pattern, next_line) 
                    and not re.match(r'^2\.3\.P$', next_line)
                    and not next_line.isdigit()):
                    title = next_line
            section_headers.append((i, page_num, line, title))

    sections = []
    for idx, (start_idx, start_page, sec_id, title) in enumerate(section_headers):
        end_idx = section_headers[idx+1][0] if idx+1 < len(section_headers) else len(all_lines)
        end_page = section_headers[idx+1][1] if idx+1 < len(section_headers) else float('inf')

        # æå–æ­£æ–‡ï¼ˆä¸å«å­ç« èŠ‚ï¼‰
        text_lines = []
        for j in range(start_idx + 1, end_idx):
            _, line = all_lines[j]
            if re.match(section_id_pattern, line):
                break
            text_lines.append(line)
        raw_text = "\n".join(text_lines).strip()

        # æå–è¡¨æ ¼ï¼ˆæŒ‰é¡µç ï¼‰
        tables = []
        for page_num, _, page_tables in pages:
            if start_page <= page_num <= end_page:
                for tbl in page_tables:
                    if tbl and len(tbl) > 1:
                        tables.append(tbl)

        sections.append({
            "id": sec_id,
            "title": title,
            "raw_text": raw_text,
            "tables": tables,
            "start_page": start_page,
            "end_page": end_page
        })

    return sections


def build_section_tree(flat_sections):
    """æ„å»ºé€’å½’ç« èŠ‚æ ‘"""
    # æŒ‰ id æ’åºï¼ˆç¡®ä¿çˆ¶èŠ‚ç‚¹åœ¨å‰ï¼‰
    flat_sections.sort(key=lambda x: x["id"])
    
    root = {"children": []}
    node_map = {}

    for sec in flat_sections:
        level = len(sec["id"].split('.'))
        content = extract_main_content(sec["raw_text"])
        has_table = len(sec["tables"]) > 0
        has_example = "ã€ç¤ºä¾‹ã€‘" in sec["raw_text"]
        has_concern = "ã€å…³æ³¨ç‚¹ã€‘" in sec["raw_text"]
        references = extract_references(sec["raw_text"])

        node = {
            "id": sec["id"],
            "title": sec["title"],
            "level": level,
            "type": "section",
            "content": content,
            "has_table": has_table,
            "has_example": has_example,
            "has_concern": has_concern,
            "references": references,
            "children": []
        }
        node_map[sec["id"]] = node

        # æ‰¾çˆ¶èŠ‚ç‚¹
        parent_id = find_parent_id(sec["id"])
        if parent_id and parent_id in node_map:
            node_map[parent_id]["children"].append(node)
        else:
            root["children"].append(node)

    return root["children"]


def extract_main_content(text: str) -> str:
    """æå–æ­£æ–‡ï¼ˆç§»é™¤ã€å…³æ³¨ç‚¹ã€‘ã€ç¤ºä¾‹ã€‘åŠä¹‹åå†…å®¹ï¼‰"""
    for marker in ["ã€å…³æ³¨ç‚¹ã€‘", "ã€ç¤ºä¾‹ã€‘"]:
        if marker in text:
            text = text.split(marker)[0].strip()
    return text


def extract_references(text: str) -> list:
    """æå–å†…éƒ¨å¼•ç”¨ï¼ˆå¦‚â€œå‚ç…§ 2.3.P.5.3â€ï¼‰"""
    refs = re.findall(r'å‚ç…§\s*(2\.3\.P\.\d+(?:\.\d+)*)', text)
    return sorted(list(set(refs)))


def find_parent_id(sec_id: str) -> str:
    """æ ¹æ®ç« èŠ‚IDæ‰¾çˆ¶IDï¼ˆå¦‚ 2.3.P.2.1.1 â†’ 2.3.P.2.1ï¼‰"""
    parts = sec_id.split('.')
    if len(parts) > 4:  # 2.3.P.x ä¸ºL3ï¼Œx.xä¸ºL4+
        return '.'.join(parts[:-1])
    return ""


if __name__ == "__main__":
    main()